{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic to automatically update imports if functions in utils are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stores_train = pd.read_csv(\"data/stores_train.csv\")\n",
    "stores_test = pd.read_csv(\"data/stores_test.csv\")\n",
    "stores_extra = pd.read_csv(\"data/stores_extra.csv\")\n",
    "stores_train, stores_val = train_test_split(stores_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_plaace_cat\n",
    "\n",
    "stores_train = split_plaace_cat(stores_train)\n",
    "stores_val = split_plaace_cat(stores_val)\n",
    "stores_extra = split_plaace_cat(stores_extra)\n",
    "stores_test = split_plaace_cat(stores_test)\n",
    "\n",
    "\n",
    "\n",
    "store_dataframes = {\n",
    "    \"train\": stores_train, \n",
    "    \"val\": stores_val, \n",
    "    \"extra\": stores_extra, \n",
    "    \"test\": stores_test, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_geographical_columns, create_chain_and_mall_columns, generate_rev_dict\n",
    "\n",
    "chain_count = stores_train[\"chain_name\"].value_counts().to_dict()\n",
    "lower_limit = 10\n",
    "rev_dict, mean_revenue = generate_rev_dict(stores_train)\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    df = create_geographical_columns(df)\n",
    "    df = create_chain_and_mall_columns(df, chain_count, lower_limit=lower_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import concat_df_keep_unq_index\n",
    "concat_df = concat_df_keep_unq_index(stores_train, stores_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!NB next cell takes 15 minutes to run (if comp_plaace_gran. = [1, 2, 3, 4]) (on M1 Mac with 16GB RAM)\n",
    "\n",
    "If you have the .csv files temp_data/closest_comp_\\{df_name\\}, skip running this cell and run the cell below it instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_dist_to_nearest_comp\n",
    "\n",
    "nearest_comp_plaace_cat_gran = [3, 4]\n",
    "n_nearest_comp = [1]\n",
    "\n",
    "store_dataframes[\"train\"], cp = find_dist_to_nearest_comp(\n",
    "    store_dataframes[\"train\"], \n",
    "    nearest_comp_plaace_cat_gran, \n",
    "    n_nearest_comp, \n",
    "    training=True, \n",
    "    training_df=concat_df,\n",
    "    _mean = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10287, 26)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_dataframes[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_cols = [\"store_id\"] + list(store_dataframes[\"train\"].columns[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['store_id',\n",
       " 'sum_dist_to_nearest_1_comp_plaace_3',\n",
       " 'sum_dist_to_nearest_1_comp_plaace_4']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_comp_train = pd.read_csv(\"temp_data/closest_comp_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_comp_train_new = store_dataframes[\"train\"][dist_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    merge_df = pd.read_csv(f\"temp_data/closest_comp_{df_name}.csv\")\n",
    "    store_dataframes[df_name] = df.merge(merge_df, left_on=\"store_id\", right_on=\"store_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!NB next cell takes 15 minutes to run. Grab a coffee or somtething while you wait :) (on M1 Mac with 16GB RAM)\n",
    "\n",
    "If you have the .csv files temp_data/closest_bus_stop_\\{df_name\\}, skip running this cell and run the cell below it instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bus_utils import find_closest_bus_stop\n",
    "\n",
    "bus_stop_n = [1, 3, 5, 7]\n",
    "bus_mean = True\n",
    "bus_sum = True\n",
    "\n",
    "bus_stop_columns = []\n",
    "\n",
    "if(bus_sum):\n",
    "    bus_stop_columns += [f\"closest_bus_stop_sum_{i}\" for i in bus_stop_n]\n",
    "\n",
    "if(bus_mean):\n",
    "    bus_stop_columns += [f\"closest_bus_stop_mean_{i}\" for i in bus_stop_n]\n",
    "\n",
    "\n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    store_dataframes[df_name] = find_closest_bus_stop(df, bus_stop_n, _sum=bus_sum, _mean=bus_mean)    \n",
    "    filepath = Path(f\"temp_data/closest_bus_stop_{df_name}.csv\")  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "    df.to_csv(filepath, columns=[\"store_id\"] + bus_stop_columns, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    merge_df = pd.read_csv(f\"temp_data/closest_comp_{df_name}.csv\")\n",
    "    bus_stop_columns = list(merge_df.columns)\n",
    "    store_dataframes[df_name] = df.merge(merge_df, left_on=\"store_id\", right_on=\"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grunnkrets import make_grunnkrets_df\n",
    "\n",
    "full_population_dataframes = {}\n",
    "\n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    full_population_dataframes[df_name] = make_grunnkrets_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = df.merge(\n",
    "        full_population_dataframes[df_name], \n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"outer\", \n",
    "        suffixes=('', '_redundant')\n",
    "    )\n",
    "    store_dataframes[df_name].drop(store_dataframes[df_name].filter(regex='_redundant$').columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fylke_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"fylke.\")]\n",
    "kommune_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"kommune.\")]\n",
    "delomrade_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"delomrade.\")]\n",
    "grunnkrets_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"grunnkrets_id.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_dist_to_nearest_comp\n",
    "\n",
    "nearest_comp_plaace_cat_gran = [3, 4]\n",
    "n_nearest_comp = [1, 2, 3]\n",
    "\n",
    "store_dataframes[\"train\"] = find_dist_to_nearest_comp(\n",
    "    store_dataframes[\"train\"], \n",
    "    nearest_comp_plaace_cat_gran, \n",
    "    n_nearest_comp, \n",
    "    training=True, \n",
    "    training_df=store_dataframes[\"train\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "OE_categorical_features = [\"bounded_chain_name\", \"kommune\", \"delomrade\", \"is_grocery\", \"plaace_cat_3\", \"plaace_cat_4\"]\n",
    "OE_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\")),\n",
    "        (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "OH_categorical_features = [\"fylke\", \"plaace_cat_2\"]\n",
    "OH_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "numerical_features = [\"lat\", \"lon\", \n",
    "\"mean_revenue_1\", \"mean_revenue_2\", \"mean_revenue_3\", \"mean_revenue_4\", \n",
    "] + delomrade_relevant_features + bus_stop_columns\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True))]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"oe_cat\", OE_categorical_transformer, OE_categorical_features),\n",
    "        (\"oh_cat\", OH_categorical_transformer, OH_categorical_features),\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "X_train = preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "X_val = preprocessor.transform(store_dataframes[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tdt4173')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b33d5bb6aa9988fd454a923e38ef42550e1626b613fe9fb888665b5922e892"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
