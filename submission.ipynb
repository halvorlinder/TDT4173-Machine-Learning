{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic to automatically update imports if functions in utils are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_with_whole_train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineer (option 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stores_train = pd.read_csv(\"data/stores_train.csv\")\n",
    "stores_test = pd.read_csv(\"data/stores_test.csv\")\n",
    "stores_extra = pd.read_csv(\"data/stores_extra.csv\")\n",
    "if not submission_with_whole_train:\n",
    "    stores_train, stores_val = train_test_split(stores_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_plaace_cat\n",
    "\n",
    "\n",
    "store_dataframes = {\n",
    "    \"train\": stores_train, \n",
    "    \"extra\": stores_extra, \n",
    "    \"test\": stores_test, \n",
    "    }\n",
    "\n",
    "if not submission_with_whole_train:\n",
    "    store_dataframes[\"val\"] = stores_val\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    df = split_plaace_cat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dataframes[\"train\"][\"log_revenue\"] = store_dataframes[\"train\"].revenue.apply(lambda x: np.log1p(x))\n",
    "if not submission_with_whole_train:\n",
    "    store_dataframes[\"val\"][\"log_revenue\"] = store_dataframes[\"val\"].revenue.apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 46.78it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 19.74it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 236.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import mean_rev_of_competitor, log_mean_rev_of_competitor, create_geographical_columns, create_chain_and_mall_columns, generate_chain_rev_dict, generate_plaace_rev_dict, create_mean_chain_rev_col\n",
    "\n",
    "chain_count = stores_train[\"chain_name\"].value_counts().to_dict()\n",
    "\n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    store_dataframes[df_name] = create_geographical_columns(df)\n",
    "    store_dataframes[df_name] = create_chain_and_mall_columns(df, chain_count, lower_limit=1)\n",
    "\n",
    "chain_rev_dict, log_bounded_chain_rev_dict = generate_chain_rev_dict(store_dataframes[\"train\"], quantile=0)\n",
    "\n",
    "for i in tqdm(range(1, 5)):\n",
    "    rev_plaace_dict, mean_plaace_revenue, log_rev_plaace_dict, log_mean_plaace_revenue = generate_plaace_rev_dict(store_dataframes[\"train\"], i, quantile=0)\n",
    "    for df_name, df in store_dataframes.items():\n",
    "            store_dataframes[df_name] = mean_rev_of_competitor(store_dataframes[df_name], i, rev_dict=rev_plaace_dict, mean_revenue=mean_plaace_revenue)\n",
    "            store_dataframes[df_name] = log_mean_rev_of_competitor(store_dataframes[df_name], i, log_rev_dict=log_rev_plaace_dict, log_mean_revenue=log_mean_plaace_revenue)\n",
    "    \n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    store_dataframes[df_name] = create_mean_chain_rev_col(df, bounded_chain_revs=chain_rev_dict, log_bounded_chain_revs=log_bounded_chain_rev_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import concat_df_keep_unq_index\n",
    "concat_df = concat_df_keep_unq_index(store_dataframes[\"train\"], store_dataframes[\"val\"])\n",
    "concat_df = concat_df_keep_unq_index(concat_df, store_dataframes[\"extra\"])\n",
    "concat_df = concat_df_keep_unq_index(concat_df, store_dataframes[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_dist_to_nearest_comp\n",
    "\n",
    "nearest_comp_plaace_cat_gran = [1, 2, 3, 4]\n",
    "n_nearest_comp = [1, 2, 3, 4, 5, 7, 10]\n",
    "\n",
    "store_dataframes[\"train\"] = find_dist_to_nearest_comp(\n",
    "    store_dataframes[\"train\"], \n",
    "    nearest_comp_plaace_cat_gran, \n",
    "    n_nearest_comp, \n",
    "    training=True, \n",
    "    training_df=concat_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not submission_with_whole_train:\n",
    "    store_dataframes[\"val\"] = find_dist_to_nearest_comp(\n",
    "        store_dataframes[\"val\"], \n",
    "        nearest_comp_plaace_cat_gran, \n",
    "        n_nearest_comp, \n",
    "        training=True, \n",
    "        training_df=concat_df,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dataframes[\"test\"] = find_dist_to_nearest_comp(\n",
    "    store_dataframes[\"test\"], \n",
    "    nearest_comp_plaace_cat_gran, \n",
    "    n_nearest_comp, \n",
    "    training=True,\n",
    "    training_df=concat_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plaace_cols = list(store_dataframes[\"train\"].columns[-56:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dict = store_dataframes[\"train\"][['log_revenue'] + comp_plaace_cols].corr().iloc[0].to_dict()\n",
    "sorted_relevant_dist_cols = [[k, v] for k, v in sorted(dist_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_relevant_cols = sorted_relevant_dist_cols[1:14:2]\n",
    "comp_relevant_cols = [r[0] for r in comp_relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comp_relevant_cols = [\n",
    "    'sum_dist_to_nearest_10_comp_plaace_1',\n",
    "    'sum_dist_to_nearest_3_comp_plaace_1',\n",
    "    'sum_dist_to_nearest_1_comp_plaace_1', \n",
    "    'sum_dist_to_nearest_3_comp_plaace_2',\n",
    "    'sum_dist_to_nearest_1_comp_plaace_2',\n",
    "    'sum_dist_to_nearest_1_comp_plaace_3',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [12:07<00:00, 181.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from bus_utils import find_closest_bus_stop\n",
    "\n",
    "bus_stop_n = [1, 2, 3, 5, 7, 10, 15 ,25, 50, 100]\n",
    "bus_mean = True\n",
    "bus_sum = True\n",
    "\n",
    "bus_stop_columns = []\n",
    "\n",
    "if(bus_sum):\n",
    "    bus_stop_columns += [f\"closest_bus_stop_sum_{i}\" for i in bus_stop_n]\n",
    "\n",
    "if(bus_mean):\n",
    "    bus_stop_columns += [f\"closest_bus_stop_mean_{i}\" for i in bus_stop_n]\n",
    "\n",
    "\n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    store_dataframes[df_name] = find_closest_bus_stop(df, bus_stop_n, _sum=bus_sum, _mean=bus_mean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_dict = store_dataframes[\"train\"][['log_revenue'] + bus_stop_columns].corr().iloc[0].to_dict()\n",
    "bus_sorted_relevant_dist_cols = [[k, v] for k, v in sorted(bus_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_relevant_cols = bus_sorted_relevant_dist_cols[1::2]\n",
    "bus_relevant_cols = [r[0] for r in bus_relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_relevant_cols = bus_relevant_cols[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:06<00:18,  6.20s/it]/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      " 50%|█████     | 2/4 [00:20<00:22, 11.15s/it]/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      " 75%|███████▌  | 3/4 [00:26<00:08,  8.51s/it]/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets_old.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "100%|██████████| 4/4 [00:28<00:00,  7.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from grunnkrets_old import make_grunnkrets_df\n",
    "\n",
    "full_population_dataframes = {}\n",
    "full_pop_columns = []\n",
    "\n",
    "# new_stores_train = pd.read_csv(\"data/stores_train.csv\")\n",
    "# new_stores_test = pd.read_csv(\"data/stores_test.csv\")\n",
    "# new_stores_extra = pd.read_csv(\"data/stores_extra.csv\")\n",
    "# if not submission_with_whole_train:\n",
    "#     new_stores_train, new_stores_val = train_test_split(new_stores_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# new_store_dataframes = {\n",
    "#     \"train\": stores_train, \n",
    "#     \"extra\": stores_extra, \n",
    "#     \"test\": stores_test, \n",
    "#     }\n",
    "\n",
    "# if not submission_with_whole_train:\n",
    "#     new_store_dataframes[\"val\"] = new_stores_val\n",
    "\n",
    "# for df_name, df in new_store_dataframes.items():\n",
    "#     df = split_plaace_cat(df)\n",
    "\n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    full_population_dataframes[df_name] = make_grunnkrets_df(df)\n",
    "    full_pop_columns = full_population_dataframes[df_name].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_columns = list(full_pop_columns[-184:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grunnkrets_id.age_0-6',\n",
       " 'grunnkrets_id.age_7-13',\n",
       " 'grunnkrets_id.age_14-20',\n",
       " 'grunnkrets_id.age_21-27',\n",
       " 'grunnkrets_id.age_28-34',\n",
       " 'grunnkrets_id.age_35-41',\n",
       " 'grunnkrets_id.age_42-48',\n",
       " 'grunnkrets_id.age_49-55',\n",
       " 'grunnkrets_id.age_56-62',\n",
       " 'grunnkrets_id.age_63-69',\n",
       " 'grunnkrets_id.age_70-76',\n",
       " 'grunnkrets_id.age_77-83',\n",
       " 'grunnkrets_id.age_84-90',\n",
       " 'delomrade.age_0-6',\n",
       " 'delomrade.age_7-13',\n",
       " 'delomrade.age_14-20',\n",
       " 'delomrade.age_21-27',\n",
       " 'delomrade.age_28-34',\n",
       " 'delomrade.age_35-41',\n",
       " 'delomrade.age_42-48',\n",
       " 'delomrade.age_49-55',\n",
       " 'delomrade.age_56-62',\n",
       " 'delomrade.age_63-69',\n",
       " 'delomrade.age_70-76',\n",
       " 'delomrade.age_77-83',\n",
       " 'delomrade.age_84-90',\n",
       " 'kommune.age_0-6',\n",
       " 'kommune.age_7-13',\n",
       " 'kommune.age_14-20',\n",
       " 'kommune.age_21-27',\n",
       " 'kommune.age_28-34',\n",
       " 'kommune.age_35-41',\n",
       " 'kommune.age_42-48',\n",
       " 'kommune.age_49-55',\n",
       " 'kommune.age_56-62',\n",
       " 'kommune.age_63-69',\n",
       " 'kommune.age_70-76',\n",
       " 'kommune.age_77-83',\n",
       " 'kommune.age_84-90',\n",
       " 'fylke.age_0-6',\n",
       " 'fylke.age_7-13',\n",
       " 'fylke.age_14-20',\n",
       " 'fylke.age_21-27',\n",
       " 'fylke.age_28-34',\n",
       " 'fylke.age_35-41',\n",
       " 'fylke.age_42-48',\n",
       " 'fylke.age_49-55',\n",
       " 'fylke.age_56-62',\n",
       " 'fylke.age_63-69',\n",
       " 'fylke.age_70-76',\n",
       " 'fylke.age_77-83',\n",
       " 'fylke.age_84-90',\n",
       " 'grunnkrets_id.c_age_0-18',\n",
       " 'grunnkrets_id.c_age_19-30',\n",
       " 'grunnkrets_id.c_age_31-55',\n",
       " 'grunnkrets_id.c_age_56-90',\n",
       " 'delomrade.c_age_0-18',\n",
       " 'delomrade.c_age_19-30',\n",
       " 'delomrade.c_age_31-55',\n",
       " 'delomrade.c_age_56-90',\n",
       " 'kommune.c_age_0-18',\n",
       " 'kommune.c_age_19-30',\n",
       " 'kommune.c_age_31-55',\n",
       " 'kommune.c_age_56-90',\n",
       " 'fylke.c_age_0-18',\n",
       " 'fylke.c_age_19-30',\n",
       " 'fylke.c_age_31-55',\n",
       " 'fylke.c_age_56-90',\n",
       " 'grunnkrets_id.mean_age',\n",
       " 'delomrade.mean_age',\n",
       " 'kommune.mean_age',\n",
       " 'fylke.mean_age',\n",
       " 'grunnkrets_id.tot_pop',\n",
       " 'delomrade.tot_pop',\n",
       " 'kommune.tot_pop',\n",
       " 'fylke.tot_pop',\n",
       " 'grunnkrets_id.c_age_0-18_ratio',\n",
       " 'grunnkrets_id.c_age_19-30_ratio',\n",
       " 'grunnkrets_id.c_age_31-55_ratio',\n",
       " 'grunnkrets_id.c_age_56-90_ratio',\n",
       " 'delomrade.c_age_0-18_ratio',\n",
       " 'delomrade.c_age_19-30_ratio',\n",
       " 'delomrade.c_age_31-55_ratio',\n",
       " 'delomrade.c_age_56-90_ratio',\n",
       " 'kommune.c_age_0-18_ratio',\n",
       " 'kommune.c_age_19-30_ratio',\n",
       " 'kommune.c_age_31-55_ratio',\n",
       " 'kommune.c_age_56-90_ratio',\n",
       " 'fylke.c_age_0-18_ratio',\n",
       " 'fylke.c_age_19-30_ratio',\n",
       " 'fylke.c_age_31-55_ratio',\n",
       " 'fylke.c_age_56-90_ratio',\n",
       " 'grunnkrets_id.tot_pop_log',\n",
       " 'delomrade.tot_pop_log',\n",
       " 'kommune.tot_pop_log',\n",
       " 'fylke.tot_pop_log',\n",
       " 'grunnkrets_id.area_km2',\n",
       " 'delomrade.area_km2',\n",
       " 'kommune.area_km2',\n",
       " 'fylke.area_km2',\n",
       " 'grunnkrets_id.couple_children_0_to_5_years',\n",
       " 'grunnkrets_id.couple_children_18_or_above',\n",
       " 'grunnkrets_id.couple_children_6_to_17_years',\n",
       " 'grunnkrets_id.couple_without_children',\n",
       " 'grunnkrets_id.single_parent_children_0_to_5_years',\n",
       " 'grunnkrets_id.single_parent_children_18_or_above',\n",
       " 'grunnkrets_id.single_parent_children_6_to_17_years',\n",
       " 'grunnkrets_id.singles',\n",
       " 'delomrade.couple_children_0_to_5_years',\n",
       " 'delomrade.couple_children_18_or_above',\n",
       " 'delomrade.couple_children_6_to_17_years',\n",
       " 'delomrade.couple_without_children',\n",
       " 'delomrade.single_parent_children_0_to_5_years',\n",
       " 'delomrade.single_parent_children_18_or_above',\n",
       " 'delomrade.single_parent_children_6_to_17_years',\n",
       " 'delomrade.singles',\n",
       " 'kommune.couple_children_0_to_5_years',\n",
       " 'kommune.couple_children_18_or_above',\n",
       " 'kommune.couple_children_6_to_17_years',\n",
       " 'kommune.couple_without_children',\n",
       " 'kommune.single_parent_children_0_to_5_years',\n",
       " 'kommune.single_parent_children_18_or_above',\n",
       " 'kommune.single_parent_children_6_to_17_years',\n",
       " 'kommune.singles',\n",
       " 'fylke.couple_children_0_to_5_years',\n",
       " 'fylke.couple_children_18_or_above',\n",
       " 'fylke.couple_children_6_to_17_years',\n",
       " 'fylke.couple_without_children',\n",
       " 'fylke.single_parent_children_0_to_5_years',\n",
       " 'fylke.single_parent_children_18_or_above',\n",
       " 'fylke.single_parent_children_6_to_17_years',\n",
       " 'fylke.singles',\n",
       " 'grunnkrets_id.tot_household',\n",
       " 'delomrade.tot_household',\n",
       " 'kommune.tot_household',\n",
       " 'fylke.tot_household',\n",
       " 'grunnkrets_id.all_households_income',\n",
       " 'grunnkrets_id.singles_income',\n",
       " 'grunnkrets_id.couple_without_children_income',\n",
       " 'grunnkrets_id.couple_with_children_income',\n",
       " 'grunnkrets_id.other_households_income',\n",
       " 'grunnkrets_id.single_parent_with_children_income',\n",
       " 'delomrade.all_households_income',\n",
       " 'delomrade.singles_income',\n",
       " 'delomrade.couple_without_children_income',\n",
       " 'delomrade.couple_with_children_income',\n",
       " 'delomrade.other_households_income',\n",
       " 'delomrade.single_parent_with_children_income',\n",
       " 'kommune.all_households_income',\n",
       " 'kommune.singles_income',\n",
       " 'kommune.couple_without_children_income',\n",
       " 'kommune.couple_with_children_income',\n",
       " 'kommune.other_households_income',\n",
       " 'kommune.single_parent_with_children_income',\n",
       " 'fylke.all_households_income',\n",
       " 'fylke.singles_income',\n",
       " 'fylke.couple_without_children_income',\n",
       " 'fylke.couple_with_children_income',\n",
       " 'fylke.other_households_income',\n",
       " 'fylke.single_parent_with_children_income',\n",
       " 'grunnkrets_id.total_income',\n",
       " 'grunnkrets_id.total_income_log',\n",
       " 'delomrade.total_income',\n",
       " 'delomrade.total_income_log',\n",
       " 'kommune.total_income',\n",
       " 'kommune.total_income_log',\n",
       " 'fylke.total_income',\n",
       " 'fylke.total_income_log',\n",
       " 'grunnkrets_id.income_density',\n",
       " 'grunnkrets_id.income_density_log',\n",
       " 'delomrade.income_density',\n",
       " 'delomrade.income_density_log',\n",
       " 'kommune.income_density',\n",
       " 'kommune.income_density_log',\n",
       " 'fylke.income_density',\n",
       " 'fylke.income_density_log',\n",
       " 'grunnkrets_id.pop_density',\n",
       " 'grunnkrets_id.pop_density_log',\n",
       " 'delomrade.pop_density',\n",
       " 'delomrade.pop_density_log',\n",
       " 'kommune.pop_density',\n",
       " 'kommune.pop_density_log',\n",
       " 'fylke.pop_density',\n",
       " 'fylke.pop_density_log']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pop_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = df.merge(\n",
    "        full_population_dataframes[df_name], \n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"outer\", \n",
    "        suffixes=('', '_redundant')\n",
    "    )\n",
    "    store_dataframes[df_name].drop(store_dataframes[df_name].filter(regex='_redundant$').columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_dict = store_dataframes[\"train\"][['log_revenue'] + full_pop_columns].corr().iloc[0].to_dict()\n",
    "full_pop_sorted_relevant_dist_cols = [[k, v] for k, v in sorted(full_pop_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_relevant_cols = full_pop_sorted_relevant_dist_cols[1:8]\n",
    "full_pop_relevant_cols = [r[0] for r in full_pop_relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fylke_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"fylke.\")]\n",
    "kommune_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"kommune.\")]\n",
    "delomrade_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"delomrade.\")]\n",
    "grunnkrets_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"grunnkrets_id.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n"
     ]
    }
   ],
   "source": [
    "from num_stores import add_num_stores_info\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = add_num_stores_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_store_cols = list(store_dataframes[\"train\"].columns[-64:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['log_revenue', 1.0],\n",
       " ['fylke.plaace_cat_1_per_capita', 0.3036462055602537],\n",
       " ['fylke.plaace_cat_1_per_tot_income', 0.29950542010049447],\n",
       " ['kommune.plaace_cat_1_per_capita', 0.25661756586529566],\n",
       " ['kommune.plaace_cat_1_per_tot_income', 0.245370813652734],\n",
       " ['fylke.plaace_cat_1_count', 0.18265093811288075],\n",
       " ['fylke.plaace_cat_4_per_capita', 0.16374528532317106],\n",
       " ['fylke.plaace_cat_4_per_tot_income', 0.16261122183977433],\n",
       " ['fylke.plaace_cat_3_per_capita', 0.1510299207483036],\n",
       " ['fylke.plaace_cat_3_per_tot_income', 0.14797391353258624],\n",
       " ['grunnkrets_id.plaace_cat_1_count', 0.1230872564366986],\n",
       " ['delomrade.plaace_cat_1_count', 0.11156949047890931],\n",
       " ['kommune.plaace_cat_1_per_km2', 0.08386190923409534],\n",
       " ['fylke.plaace_cat_4_count', 0.07989315137361924],\n",
       " ['fylke.plaace_cat_3_count', 0.07410923091607406],\n",
       " ['kommune.plaace_cat_3_per_capita', 0.06780469618921373],\n",
       " ['grunnkrets_id.plaace_cat_1_per_capita', 0.06542117839938809],\n",
       " ['delomrade.plaace_cat_1_per_capita', 0.06534119919978355],\n",
       " ['kommune.plaace_cat_4_per_capita', 0.06456637240824716],\n",
       " ['kommune.plaace_cat_3_per_tot_income', 0.05933152266233765],\n",
       " ['delomrade.plaace_cat_1_per_km2', 0.05902346572030938],\n",
       " ['kommune.plaace_cat_1_count', 0.05818730449207609],\n",
       " ['kommune.plaace_cat_4_per_tot_income', 0.05617954407113872],\n",
       " ['fylke.plaace_cat_1_per_km2', 0.054663516691600994],\n",
       " ['grunnkrets_id.plaace_cat_1_per_tot_income', 0.05429518720324642],\n",
       " ['delomrade.plaace_cat_1_per_tot_income', 0.05120721772847184],\n",
       " ['grunnkrets_id.plaace_cat_2_count', 0.04342873619429535],\n",
       " ['grunnkrets_id.plaace_cat_1_per_km2', 0.039063195576337674],\n",
       " ['delomrade.plaace_cat_3_count', 0.03867636847454147],\n",
       " ['grunnkrets_id.plaace_cat_3_count', 0.037196977864265054],\n",
       " ['delomrade.plaace_cat_3_per_capita', 0.034019562809161165],\n",
       " ['delomrade.plaace_cat_3_per_tot_income', 0.03167836734839887],\n",
       " ['delomrade.plaace_cat_2_count', 0.031576955629162254],\n",
       " ['grunnkrets_id.plaace_cat_3_per_capita', 0.03009024777705766],\n",
       " ['grunnkrets_id.plaace_cat_3_per_tot_income', 0.029124769759925357],\n",
       " ['delomrade.plaace_cat_2_per_capita', 0.02663206168317343],\n",
       " ['grunnkrets_id.plaace_cat_2_per_capita', 0.026145182301458146],\n",
       " ['fylke.plaace_cat_2_per_tot_income', 0.025831280006826805],\n",
       " ['delomrade.plaace_cat_3_per_km2', 0.024970924303482665],\n",
       " ['delomrade.plaace_cat_2_per_tot_income', 0.02466237791909166],\n",
       " ['grunnkrets_id.plaace_cat_2_per_tot_income', 0.024197652104448273],\n",
       " ['delomrade.plaace_cat_2_per_km2', 0.02417609814428057],\n",
       " ['fylke.plaace_cat_2_per_capita', 0.0239510170369684],\n",
       " ['grunnkrets_id.plaace_cat_4_per_km2', -0.021315764990545823],\n",
       " ['kommune.plaace_cat_3_per_km2', 0.017122752175493187],\n",
       " ['fylke.plaace_cat_3_per_km2', 0.016834603013333792],\n",
       " ['kommune.plaace_cat_3_count', 0.015029712827820759],\n",
       " ['delomrade.plaace_cat_4_per_capita', 0.013270740243254173],\n",
       " ['grunnkrets_id.plaace_cat_4_per_capita', 0.012920119862730312],\n",
       " ['delomrade.plaace_cat_4_per_tot_income', 0.011647864592422584],\n",
       " ['grunnkrets_id.plaace_cat_4_per_tot_income', 0.01103648442072455],\n",
       " ['grunnkrets_id.plaace_cat_4_count', 0.010172899587287145],\n",
       " ['kommune.plaace_cat_2_count', 0.01003955531561964],\n",
       " ['grunnkrets_id.plaace_cat_3_per_km2', 0.008896198724637617],\n",
       " ['grunnkrets_id.plaace_cat_2_per_km2', 0.008442780940558295],\n",
       " ['fylke.plaace_cat_2_per_km2', 0.007314983139576939],\n",
       " ['kommune.plaace_cat_2_per_tot_income', 0.0068516675413848194],\n",
       " ['fylke.plaace_cat_2_count', 0.006472781951590467],\n",
       " ['kommune.plaace_cat_2_per_capita', 0.006333978483626561],\n",
       " ['fylke.plaace_cat_4_per_km2', 0.006200765923411325],\n",
       " ['kommune.plaace_cat_2_per_km2', 0.005454350588733066],\n",
       " ['delomrade.plaace_cat_4_per_km2', -0.004786398854204821],\n",
       " ['delomrade.plaace_cat_4_count', 0.003731892476898393],\n",
       " ['kommune.plaace_cat_4_count', -0.0013682820801888314],\n",
       " ['kommune.plaace_cat_4_per_km2', 0.0009298632119785561]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_store_dict = store_dataframes[\"train\"][['log_revenue'] + num_store_cols].corr().iloc[0].to_dict()\n",
    "num_store_sorted_relevant_cols = [[k, v] for k, v in sorted(num_store_dict.items(), key=lambda item: abs(item[1]), reverse=True)]\n",
    "num_store_sorted_relevant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_store_relevant_cols = num_store_sorted_relevant_cols[1:15]\n",
    "num_store_relevant_cols = [r[0] for r in num_store_relevant_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding mean revenue (plaace_cat, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avg_revenue import add_avg_revenue\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = add_avg_revenue(df, total=submission_with_whole_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_cols = list(store_dataframes[\"train\"].columns)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_dict = store_dataframes[\"val\"][['log_revenue'] + mean_rev_cols].corr().iloc[0].to_dict()\n",
    "mean_rev_sorted_relevant_cols = [[k, v] for k, v in sorted(mean_rev_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_relevant_cols = mean_rev_sorted_relevant_cols[1:]\n",
    "mean_rev_relevant_cols = [r[0] for r in mean_rev_relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_relevant_log_cols = []\n",
    "for col in mean_rev_relevant_cols:\n",
    "    for df_name, df in store_dataframes.items():\n",
    "        store_dataframes[df_name][col + \"_log\"] = store_dataframes[df_name][col].apply(lambda x: np.log1p(x))\n",
    "    mean_rev_relevant_log_cols.append(col + \"_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_plaace_index import create_index_csv, add_new_plaace_index\n",
    "\n",
    "create_index_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = add_new_plaace_index(store_dataframes[df_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering import add_clusters\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = add_clusters(store_dataframes[df_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name][\"chain_count\"] = store_dataframes[df_name].bounded_chain_name.apply(lambda x: 0 if (x == \"OTHER\" or x in chain_count.keys()) else chain_count[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data directly from CSV (option 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "if submission_with_whole_train:\n",
    "    stores_train = pd.read_csv(\"temp_data/full_train_features_train.csv\", index_col=0)\n",
    "    stores_extra = pd.read_csv(\"temp_data/full_train_features_extra.csv\", index_col=0)\n",
    "    stores_test = pd.read_csv(\"temp_data/full_train_features_test.csv\", index_col=0)\n",
    "\n",
    "    old_store_dataframes = {\n",
    "        \"train\": stores_train, \n",
    "        \"extra\": stores_extra, \n",
    "        \"test\": stores_test, \n",
    "        }\n",
    "else:\n",
    "    stores_train = pd.read_csv(\"temp_data/full_features_train.csv\", index_col=0)\n",
    "    stores_val = pd.read_csv(\"temp_data/full_features_val.csv\", index_col=0)\n",
    "    stores_extra = pd.read_csv(\"temp_data/full_features_extra.csv\", index_col=0)\n",
    "    stores_test = pd.read_csv(\"temp_data/full_features_test.csv\", index_col=0)\n",
    "\n",
    "    old_store_dataframes = {\n",
    "        \"train\": stores_train, \n",
    "        \"extra\": stores_extra, \n",
    "        \"test\": stores_test, \n",
    "        \"val\": stores_val\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"relevant_cols.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "comp_relevant_cols = data[0].strip().split(\",\")\n",
    "bus_relevant_cols = data[1].strip().split(\",\")\n",
    "num_store_relevant_cols = data[2].strip().split(\",\")\n",
    "full_pop_relevant_cols = data[3].strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_relevant_cols = comp_relevant_cols[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_relevant_cols += [\n",
    "    'sum_dist_to_nearest_10_comp_plaace_2',\n",
    "    'sum_dist_to_nearest_5_comp_plaace_2', \n",
    "    'sum_dist_to_nearest_10_comp_plaace_1',\n",
    "    'sum_dist_to_nearest_5_comp_plaace_1', \n",
    "    'sum_dist_to_nearest_1_comp_plaace_1',\n",
    "    'sum_dist_to_nearest_1_comp_plaace_2'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_relevant_cols = bus_relevant_cols[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fylke.plaace_cat_1_count',\n",
       " 'fylke.plaace_cat_4_count',\n",
       " 'fylke.plaace_cat_3_count',\n",
       " 'delomrade.plaace_cat_1_count',\n",
       " 'kommune.plaace_cat_1_count',\n",
       " 'grunnkrets_id.plaace_cat_1_count']"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_store_relevant_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RMSLE import rmsle\n",
    "from pred_var_utils import reverse_log1p_transform_pred_var\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "OE_categorical_features = [\"bounded_chain_name\", \"kommune\", \"delomrade\", \"is_grocery\", \"plaace_cat_2\", \"plaace_cat_3\", \"plaace_cat_4\", \"grunnkrets_id\"]#\"new_plaace\"]\n",
    "OE_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\")),\n",
    "        (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "OH_categorical_features = [\"plaace_cat_1\"] #[\"fylke\", \"plaace_cat_2\"]\n",
    "OH_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "numerical_features = [\"lat\", \"lon\", \n",
    "#\"mean_revenue_1\", \"mean_revenue_2\", \"mean_revenue_3\", \"mean_revenue_4\", \n",
    "\"log_mean_revenue_1\", \n",
    "\"log_mean_revenue_2\", \n",
    "\"log_mean_revenue_3\", \n",
    "\"log_mean_revenue_4\", \n",
    "#\"fylke.plaace_cat_1_mean_revenue_log\", #\"fylke.plaace_cat_3_mean_revenue_log\", \n",
    "#'grunnkrets_id.tot_pop',\n",
    "#'delomrade.tot_pop',\n",
    "#'kommune.tot_pop',\n",
    "#'fylke.tot_pop',\n",
    "\"log_chain_mean_revenue\"\n",
    "] + comp_relevant_cols \n",
    "#+ bus_relevant_cols \n",
    "#+ num_store_relevant_cols \n",
    "#+ full_pop_relevant_cols \n",
    "#+ mean_rev_relevant_log_cols\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True))]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "       (\"oe_cat\", OE_categorical_transformer, OE_categorical_features),\n",
    "       (\"oh_cat\", OH_categorical_transformer, OH_categorical_features),\n",
    "       (\"num\", numerical_transformer, numerical_features),\n",
    "   ],\n",
    "   remainder='drop'\n",
    ")\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"oe_cat\", OE_categorical_transformer, all_OE_cols),\n",
    "#         (\"oh_cat\", OH_categorical_transformer, all_OH_cols),\n",
    "#         (\"num\", numerical_transformer, all_num_cols[:16] + all_num_cols[72:276]),\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "\n",
    "\n",
    "X_train = preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "if not submission_with_whole_train:\n",
    "    X_val = preprocessor.transform(store_dataframes[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(store_dataframes[\"train\"].log_revenue)\n",
    "if not submission_with_whole_train:\n",
    "    y_val = np.array(store_dataframes[\"val\"].revenue)\n",
    "mean_y = y_train.mean()\n",
    "std_y = y_train.std()\n",
    "\n",
    "y_train -= mean_y\n",
    "y_train /= std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10287, 29)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlier_utils import remove_low_revenue\n",
    "\n",
    "store_dataframes[\"out_removed_train\"] = remove_low_revenue(store_dataframes[\"train\"].copy(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from remove_outlier import remove_outliers\n",
    "\n",
    "store_dataframes[\"out_removed_train\"] = remove_outliers(store_dataframes[\"out_removed_train\"], \"plaace_cat_4\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10212"
      ]
     },
     "execution_count": 1169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_dataframes[\"out_removed_train\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dataframes[\"out_removed_train\"] = find_dist_to_nearest_comp(\n",
    "    store_dataframes[\"out_removed_train\"], \n",
    "    nearest_comp_plaace_cat_gran, \n",
    "    n_nearest_comp, \n",
    "    training=True, \n",
    "    training_df=concat_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 14.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1, 5)):\n",
    "    rev_plaace_dict, mean_plaace_revenue, log_rev_plaace_dict, log_mean_plaace_revenue = generate_plaace_rev_dict(store_dataframes[\"train\"], i, quantile=0)\n",
    "    store_dataframes[\"out_removed_train\"] = mean_rev_of_competitor(store_dataframes[\"out_removed_train\"], i, rev_dict=rev_plaace_dict, mean_revenue=mean_plaace_revenue)\n",
    "    store_dataframes[\"out_removed_train\"] = log_mean_rev_of_competitor(store_dataframes[\"out_removed_train\"], i, log_rev_dict=log_rev_plaace_dict, log_mean_revenue=log_mean_plaace_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_rev_dict, log_bounded_chain_rev_dict = generate_chain_rev_dict(store_dataframes[\"out_removed_train\"], quantile=0)\n",
    "store_dataframes[\"out_removed_train\"] = create_mean_chain_rev_col(store_dataframes[\"out_removed_train\"], bounded_chain_revs=chain_rev_dict, log_bounded_chain_revs=log_bounded_chain_rev_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_cols_dict = {\n",
    "    \"OE\": [],\n",
    "    \"OH\": [],  \n",
    "    \"num\": []\n",
    "}\n",
    "\n",
    "reduntant_cols = [\"store_id\", \"year\", \"store_name\", \"plaace_hierarchy_id\", \"sales_channel_name\", \"address\", \"revenue\", \"log_revenue\", \"point\", \"plaace_cat_0\"]\n",
    "\n",
    "for col_name, dtype in store_dataframes[\"train\"].dtypes.to_dict().items():\n",
    "    if(col_name in reduntant_cols):\n",
    "        continue\n",
    "    if(dtype == int or dtype == float):\n",
    "        _type = \"num\"\n",
    "    elif(dtype == bool):\n",
    "        _type = \"OE\"\n",
    "    elif(dtype == object):\n",
    "        if(store_dataframes[\"train\"][col_name].nunique() <= 10):\n",
    "            _type = \"OH\"\n",
    "        else:\n",
    "            _type = \"OE\"\n",
    "    else:\n",
    "        print(f\"Unknown type {dtype} encountered for columns {col_name}\")\n",
    "    PCA_cols_dict[_type].append(col_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_num_cols = PCA_cols_dict[\"num\"][:339]\n",
    "PCA_cols_dict[\"OE\"] += PCA_cols_dict[\"num\"][-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plaace_cat_1']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_OH_cols = PCA_cols_dict[\"OH\"]\n",
    "all_OH_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chain_name',\n",
       " 'mall_name',\n",
       " 'plaace_cat_2',\n",
       " 'plaace_cat_3',\n",
       " 'plaace_cat_4',\n",
       " 'is_mall',\n",
       " 'is_chain',\n",
       " 'bounded_chain_name',\n",
       " 'is_grocery',\n",
       " 'old_plaace',\n",
       " 'new_plaace_name',\n",
       " 'new_plaace',\n",
       " 'cluster']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_OE_cols = PCA_cols_dict[\"OE\"]\n",
    "all_OE_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_plaace_cat_cols = all_num_cols[276:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cols_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #(\"oe_cat\", OE_categorical_transformer, PCA_cols_dict[\"OE\"]),\n",
    "        #(\"oh_cat\", OH_categorical_transformer, PCA_cols_dict[\"OH\"]),\n",
    "        (\"num\", numerical_transformer, geo_plaace_cat_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_PCA_X_train_count_cols = count_cols_preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "if not submission_with_whole_train:\n",
    "    pre_PCA_X_val_count_cols = count_cols_preprocessor.transform(store_dataframes[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "PCA_X_train = pca.fit_transform(pre_PCA_X_train_count_cols)\n",
    "if not submission_with_whole_train:\n",
    "    PCA_X_val = pca.transform(pre_PCA_X_val_count_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5916238986622017\n",
      "[0.33331616 0.15396038 0.10434736]\n",
      "[464.7750719  315.87782456 260.04911143]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,PCA_X_train),axis=1)\n",
    "if not submission_with_whole_train:\n",
    "    X_val = np.concatenate((X_val,PCA_X_val),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comp dist cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_comp_dist_cols = all_num_cols[16:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #(\"oe_cat\", OE_categorical_transformer, PCA_cols_dict[\"OE\"]),\n",
    "        #(\"oh_cat\", OH_categorical_transformer, PCA_cols_dict[\"OH\"]),\n",
    "        (\"num\", numerical_transformer, PCA_comp_dist_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_PCA_X_train_comp_cols = comp_dist_preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "if not submission_with_whole_train:\n",
    "    pre_PCA_X_val_comp_cols = comp_dist_preprocessor.transform(store_dataframes[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "PCA_X_train = pca.fit_transform(pre_PCA_X_train_comp_cols)\n",
    "if not submission_with_whole_train:\n",
    "    PCA_X_val = pca.transform(pre_PCA_X_val_comp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9591197711131739\n",
      "[0.52962483 0.26730788 0.10415261 0.03619452 0.02183993]\n",
      "[552.3604204  392.41379081 244.94775403 144.39754555 112.16672492]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,PCA_X_train),axis=1)\n",
    "if not submission_with_whole_train:\n",
    "    X_val = np.concatenate((X_val,PCA_X_val),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full population cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_cols = all_num_cols[144:276]\n",
    "full_pop_cols\n",
    "full_pop_age_cols = full_pop_cols[:20] + full_pop_cols[25:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #(\"oe_cat\", OE_categorical_transformer, PCA_cols_dict[\"OE\"]),\n",
    "        #(\"oh_cat\", OH_categorical_transformer, PCA_cols_dict[\"OH\"]),\n",
    "        (\"num\", numerical_transformer, full_pop_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_PCA_X_train_full_pop_cols = full_pop_preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "if not submission_with_whole_train:\n",
    "    pre_PCA_X_val_full_pop_cols = full_pop_preprocessor.transform(store_dataframes[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "PCA_X_train = pca.fit_transform(pre_PCA_X_train_full_pop_cols)\n",
    "if not submission_with_whole_train:\n",
    "    PCA_X_val = pca.transform(pre_PCA_X_val_full_pop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7176714472755126\n",
      "[0.40588052 0.20522538 0.10656555]\n",
      "[742.38713818 527.89416982 380.39934023]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,PCA_X_train),axis=1)\n",
    "if not submission_with_whole_train:\n",
    "    X_val = np.concatenate((X_val,PCA_X_val),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bus distance cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_cols = all_num_cols[72:92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #(\"oe_cat\", OE_categorical_transformer, PCA_cols_dict[\"OE\"]),\n",
    "        #(\"oh_cat\", OH_categorical_transformer, PCA_cols_dict[\"OH\"]),\n",
    "        (\"num\", numerical_transformer, bus_stop_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_PCA_X_train_bus_stop_cols = bus_stop_preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "if not submission_with_whole_train:\n",
    "    pre_PCA_X_val_bus_stop_cols = bus_stop_preprocessor.transform(store_dataframes[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "PCA_X_train = pca.fit_transform(pre_PCA_X_train_bus_stop_cols)\n",
    "if not submission_with_whole_train:\n",
    "    PCA_X_val = pca.transform(pre_PCA_X_val_bus_stop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9413717977937323\n",
      "[0.78820564 0.15316616]\n",
      "[402.69768744 177.51734059]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,PCA_X_train),axis=1)\n",
    "if not submission_with_whole_train:\n",
    "    X_val = np.concatenate((X_val,PCA_X_val),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean revenue cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country.plaace_cat_2_mean_revenue_log',\n",
       " 'fylke.plaace_cat_2_mean_revenue_log',\n",
       " 'country.plaace_cat_1_mean_revenue_log',\n",
       " 'fylke.plaace_cat_1_mean_revenue_log',\n",
       " 'kommune.plaace_cat_2_mean_revenue_log',\n",
       " 'kommune.plaace_cat_1_mean_revenue_log',\n",
       " 'fylke.plaace_cat_0_mean_revenue_log',\n",
       " 'kommune.plaace_cat_0_mean_revenue_log']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mean_rev_cols = []\n",
    "mean_rev_cols = PCA_cols_dict[\"num\"][366:-3]\n",
    "for col in mean_rev_cols:\n",
    "    if \"delomrade\" in col:\n",
    "        continue\n",
    "    if \"cat_4\" in col:\n",
    "        continue\n",
    "    if \"cat_3\" in col:\n",
    "        continue\n",
    "    new_mean_rev_cols.append(col)\n",
    "\n",
    "new_mean_rev_cols.remove('country.plaace_cat_0_mean_revenue_log')\n",
    "new_mean_rev_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #(\"oe_cat\", OE_categorical_transformer, PCA_cols_dict[\"OE\"]),\n",
    "        #(\"oh_cat\", OH_categorical_transformer, PCA_cols_dict[\"OH\"]),\n",
    "        (\"num\", numerical_transformer, mean_rev_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_PCA_X_train_mean_rev_cols = mean_rev_preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "if not submission_with_whole_train:\n",
    "    pre_PCA_X_val_mean_rev_cols = mean_rev_preprocessor.transform(store_dataframes[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)\n",
    "PCA_X_train = pca.fit_transform(pre_PCA_X_train_mean_rev_cols)\n",
    "if not submission_with_whole_train:\n",
    "    PCA_X_val = pca.transform(pre_PCA_X_val_mean_rev_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94808787609428\n",
      "[0.58917955 0.10935341 0.09790841 0.0548109  0.03787202 0.03114834\n",
      " 0.02781524]\n",
      "[339.34777294 146.19661838 138.33471162 103.50340937  86.03603807\n",
      "  78.02587043  73.73311963]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,PCA_X_train),axis=1)\n",
    "if not submission_with_whole_train:\n",
    "    X_val = np.concatenate((X_val,PCA_X_val),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/8hp9d_rj4_9d8vgrqztmqhf40000gn/T/ipykernel_35390/498962467.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  cor_matrix = store_dataframes[\"train\"].corr().abs()\n"
     ]
    }
   ],
   "source": [
    "cor_matrix = store_dataframes[\"train\"].corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fylke',\n",
       " 'kommune',\n",
       " 'delomrade',\n",
       " 'mean_dist_to_nearest_1_comp_plaace_1',\n",
       " 'mean_dist_to_nearest_2_comp_plaace_1',\n",
       " 'mean_dist_to_nearest_3_comp_plaace_1',\n",
       " 'mean_dist_to_nearest_4_comp_plaace_1',\n",
       " 'mean_dist_to_nearest_5_comp_plaace_1',\n",
       " 'mean_dist_to_nearest_7_comp_plaace_1',\n",
       " 'mean_dist_to_nearest_10_comp_plaace_1',\n",
       " 'mean_dist_to_nearest_1_comp_plaace_2',\n",
       " 'mean_dist_to_nearest_2_comp_plaace_2',\n",
       " 'mean_dist_to_nearest_3_comp_plaace_2',\n",
       " 'mean_dist_to_nearest_4_comp_plaace_2',\n",
       " 'mean_dist_to_nearest_5_comp_plaace_2',\n",
       " 'mean_dist_to_nearest_7_comp_plaace_2',\n",
       " 'mean_dist_to_nearest_10_comp_plaace_2',\n",
       " 'mean_dist_to_nearest_1_comp_plaace_3',\n",
       " 'mean_dist_to_nearest_2_comp_plaace_3',\n",
       " 'mean_dist_to_nearest_3_comp_plaace_3',\n",
       " 'mean_dist_to_nearest_4_comp_plaace_3',\n",
       " 'mean_dist_to_nearest_5_comp_plaace_3',\n",
       " 'mean_dist_to_nearest_7_comp_plaace_3',\n",
       " 'mean_dist_to_nearest_10_comp_plaace_3',\n",
       " 'mean_dist_to_nearest_1_comp_plaace_4',\n",
       " 'mean_dist_to_nearest_2_comp_plaace_4',\n",
       " 'mean_dist_to_nearest_3_comp_plaace_4',\n",
       " 'mean_dist_to_nearest_4_comp_plaace_4',\n",
       " 'mean_dist_to_nearest_5_comp_plaace_4',\n",
       " 'mean_dist_to_nearest_7_comp_plaace_4',\n",
       " 'mean_dist_to_nearest_10_comp_plaace_4',\n",
       " 'closest_bus_stop_mean_1',\n",
       " 'closest_bus_stop_mean_2',\n",
       " 'closest_bus_stop_mean_3',\n",
       " 'closest_bus_stop_mean_5',\n",
       " 'closest_bus_stop_mean_7',\n",
       " 'closest_bus_stop_mean_10',\n",
       " 'closest_bus_stop_mean_15',\n",
       " 'closest_bus_stop_mean_25',\n",
       " 'closest_bus_stop_mean_50',\n",
       " 'closest_bus_stop_mean_100',\n",
       " 'grunnkrets_id.age_14-20',\n",
       " 'grunnkrets_id.age_28-34',\n",
       " 'grunnkrets_id.age_42-48',\n",
       " 'grunnkrets_id.age_49-55',\n",
       " 'grunnkrets_id.age_56-62',\n",
       " 'grunnkrets_id.age_63-69',\n",
       " 'grunnkrets_id.age_70-76',\n",
       " 'grunnkrets_id.age_84-90',\n",
       " 'kommune.age_35-41',\n",
       " 'kommune.age_42-48',\n",
       " 'kommune.age_49-55',\n",
       " 'kommune.age_56-62',\n",
       " 'kommune.age_63-69',\n",
       " 'kommune.age_70-76',\n",
       " 'kommune.age_77-83',\n",
       " 'kommune.age_84-90',\n",
       " 'grunnkrets_id.c_age_0-18',\n",
       " 'grunnkrets_id.c_age_19-30',\n",
       " 'grunnkrets_id.c_age_31-55',\n",
       " 'grunnkrets_id.c_age_56-90',\n",
       " 'delomrade.c_age_0-18',\n",
       " 'kommune.c_age_0-18',\n",
       " 'kommune.c_age_19-30',\n",
       " 'kommune.c_age_31-55',\n",
       " 'kommune.c_age_56-90',\n",
       " 'fylke.c_age_19-30',\n",
       " 'fylke.c_age_56-90',\n",
       " 'grunnkrets_id.tot_pop',\n",
       " 'kommune.tot_pop',\n",
       " 'grunnkrets_id.couple_children_0_to_5_years',\n",
       " 'grunnkrets_id.couple_children_6_to_17_years',\n",
       " 'grunnkrets_id.couple_without_children',\n",
       " 'grunnkrets_id.single_parent_children_6_to_17_years',\n",
       " 'grunnkrets_id.singles',\n",
       " 'delomrade.couple_children_0_to_5_years',\n",
       " 'kommune.couple_children_0_to_5_years',\n",
       " 'kommune.couple_children_6_to_17_years',\n",
       " 'kommune.couple_without_children',\n",
       " 'kommune.single_parent_children_18_or_above',\n",
       " 'kommune.single_parent_children_6_to_17_years',\n",
       " 'kommune.singles',\n",
       " 'fylke.couple_children_0_to_5_years',\n",
       " 'grunnkrets_id.tot_household',\n",
       " 'delomrade.tot_household',\n",
       " 'kommune.tot_household',\n",
       " 'fylke.tot_household',\n",
       " 'delomrade.all_households_income',\n",
       " 'delomrade.singles_income',\n",
       " 'delomrade.couple_without_children_income',\n",
       " 'delomrade.couple_with_children_income',\n",
       " 'delomrade.other_households_income',\n",
       " 'delomrade.single_parent_with_children_income',\n",
       " 'grunnkrets_id.total_income',\n",
       " 'kommune.total_income',\n",
       " 'fylke.pop_density',\n",
       " 'country.plaace_cat_1_mean_revenue',\n",
       " 'country.plaace_cat_2_mean_revenue',\n",
       " 'country.plaace_cat_3_mean_revenue',\n",
       " 'country.plaace_cat_4_mean_revenue']"
      ]
     },
     "execution_count": 999,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.999)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# best params (8.11)\n",
    "# {'max_features': 8, 'min_samples_leaf': 4, 'min_samples_split': 32, 'n_estimators': 500}\n",
    "# rmsle(on val): 0.719099511243053\n",
    "\n",
    "rf = RandomForestRegressor(random_state=0, n_jobs=-1, max_features=8, min_samples_leaf=4, min_samples_split=32, n_estimators=500)\n",
    "rf_params = {\n",
    "    \"n_estimators\" : (100, 250, 500), \n",
    "    \"max_features\" : (2, 4, 8), \n",
    "    \"min_samples_split\" : (4, 8, 16, 32), \n",
    "    \"min_samples_leaf\" : (1, 2, 4), \n",
    "    }\n",
    "\n",
    "rf_clf = GridSearchCV(rf, rf_params, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8990312771546934"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "rf_y_pred = reverse_log1p_transform_pred_var(rf.predict(X_val), std_y=std_y, mean_y=mean_y)\n",
    "rmsle(y_val, rf_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_y_pred = reverse_log1p_transform_pred_var(rf.predict(X_val), std_y=std_y, mean_y=mean_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7431171369975103"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(y_val, rf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-29 {color: black;background-color: white;}#sk-container-id-29 pre{padding: 0;}#sk-container-id-29 div.sk-toggleable {background-color: white;}#sk-container-id-29 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-29 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-29 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-29 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-29 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-29 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-29 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-29 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-29 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-29 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-29 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-29 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-29 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-29 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-29 div.sk-item {position: relative;z-index: 1;}#sk-container-id-29 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-29 div.sk-item::before, #sk-container-id-29 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-29 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-29 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-29 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-29 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-29 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-29 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-29 div.sk-label-container {text-align: center;}#sk-container-id-29 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-29 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-29\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" checked><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_pred = reverse_log1p_transform_pred_var(lr.predict(X_val), std_y, mean_y)\n",
    "lr_y_pred = np.array([max(0, xi) for xi in lr_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7485071979848835"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(y_val, lr_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# best params (8.11)\n",
    "# {'learning_rate': 0.05, 'min_child_samples': 16, 'min_split_gain': 0, 'n_estimators': 100, 'num_leaves': 25}\n",
    "# rmsle(on val): 0.7208999886795342\n",
    "\n",
    "lgbm = LGBMRegressor(\n",
    "    random_state=0, \n",
    "    n_jobs=-1, \n",
    "    learning_rate=0.05, \n",
    "    min_child_samples=16, \n",
    "    min_split_gain=0, \n",
    "    n_estimators=100, \n",
    "    num_leaves=25\n",
    "    )\n",
    "\n",
    "lgbm_params = {\n",
    "    \"num_leaves\" : (10, 25, 31, 75), \n",
    "    \"learning_rate\" : (0.05, 0.1, 0.25),\n",
    "    \"n_estimators\" : (50, 100, 250), \n",
    "    \"min_split_gain\" : (0, 0.01), \n",
    "    \"min_child_samples\" : (4, 8, 16, 32), \n",
    "    \"reg_alpha\" : (0, 0.01, 0.1), \n",
    "    \"reg_lambda\" : (0, 0.01, 0.1), \n",
    "    }\n",
    "\n",
    "#lgbm_clf = RandomizedSearchCV(lgbm, lgbm_params, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-35 {color: black;background-color: white;}#sk-container-id-35 pre{padding: 0;}#sk-container-id-35 div.sk-toggleable {background-color: white;}#sk-container-id-35 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-35 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-35 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-35 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-35 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-35 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-35 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-35 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-35 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-35 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-35 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-35 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-35 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-35 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-35 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-35 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-35 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-35 div.sk-item {position: relative;z-index: 1;}#sk-container-id-35 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-35 div.sk-item::before, #sk-container-id-35 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-35 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-35 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-35 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-35 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-35 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-35 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-35 div.sk-label-container {text-align: center;}#sk-container-id-35 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-35 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-35\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(learning_rate=0.05, min_child_samples=16, min_split_gain=0,\n",
       "              num_leaves=25, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" checked><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(learning_rate=0.05, min_child_samples=16, min_split_gain=0,\n",
       "              num_leaves=25, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(learning_rate=0.05, min_child_samples=16, min_split_gain=0,\n",
       "              num_leaves=25, random_state=0)"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_y_pred = reverse_log1p_transform_pred_var(lgbm.predict(X_val), std_y, mean_y)\n",
    "lgbm_y_pred = np.array([max(0, xi) for xi in lgbm_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.722147096898195"
      ]
     },
     "execution_count": 1014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(y_val, lgbm_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# best params (8.11)\n",
    "# {'depth': 6, 'l2_leaf_reg': 10, 'learning_rate': 0.05, 'eval_metric': 'RMSE'}\n",
    "# rmsle(on val) = 0.0.7148919867904334\n",
    "\n",
    "# best params (12.11) (all features)\n",
    "# {'depth': 6, 'l2_leaf_reg': 10, 'learning_rate': 0.05, 'eval_metric': 'RMSE'}\n",
    "# rmsle(on val) = 0.7177413486698632\n",
    "\n",
    "cb = CatBoostRegressor(\n",
    "    random_seed=0, \n",
    "    verbose=False, \n",
    "    eval_metric=\"RMSE\", \n",
    "    rsm=0.1,\n",
    "    depth=6, \n",
    "    l2_leaf_reg= 10, \n",
    "    learning_rate= 0.05\n",
    "    )\n",
    "\n",
    "cb_params = grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.3],\n",
    "    'depth': [5, 6, 8, 10, 15, 20],\n",
    "    'l2_leaf_reg': [3, 4, 5, 6, 7, 8, 10, 15], \n",
    "    }\n",
    "\n",
    "#cb_clf = cb.randomized_search(cb_params, X=X_train, y=y_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cb_clf[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x2ad480df0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_y_pred = reverse_log1p_transform_pred_var(cb.predict(X_val), std_y, mean_y)\n",
    "cb_y_pred = np.array([max(0, xi) for xi in cb_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7140855949981019"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(y_val, cb_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7171520894142979"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(y_val, cb_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "estimators = [\n",
    "    ('rf', rf), \n",
    "    ('cb', cb), \n",
    "    ('lgbm', lgbm), \n",
    "]\n",
    "\n",
    "rf_end_params = {\n",
    "    \"n_estimators\" : (50, 100, 250), \n",
    "    \"max_features\" : (1, 2, 3), \n",
    "    \"min_samples_split\" : (16, 32), \n",
    "    \"min_samples_leaf\" : (2, 4, 8), \n",
    "    }\n",
    "\n",
    "rf_end = RandomForestRegressor(random_state=0, n_jobs=-1, n_estimators=50, max_features=3)\n",
    "\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=rf_end\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_y_pred = reverse_log1p_transform_pred_var(reg.predict(X_val), std_y, mean_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle(y_pred=reg_y_pred, y_true=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set \n",
    "X_test = preprocessor.transform(store_dataframes[\"test\"])\n",
    "y_test_pred = reverse_log1p_transform_pred_var(cb.predict(X_test), std_y, mean_y)\n",
    "\n",
    "# Generate submission dataframe \n",
    "# NOTE: It is important that the ID and predicted values match\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = stores_test.store_id \n",
    "submission['predicted'] = np.asarray(y_test_pred)\n",
    "\n",
    "# Save it to disk (`index=False` means don't save the index in the csv)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    filepath = Path(f\"temp_data/full_features_{df_name}_13_nov.csv\")  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "    df.to_csv(filepath, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tdt4173')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b33d5bb6aa9988fd454a923e38ef42550e1626b613fe9fb888665b5922e892"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
