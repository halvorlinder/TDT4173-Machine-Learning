{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic to automatically update imports if functions in utils are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stores_train = pd.read_csv(\"data/stores_train.csv\")\n",
    "stores_test = pd.read_csv(\"data/stores_test.csv\")\n",
    "stores_train, stores_val = train_test_split(stores_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stores_train.shape)\n",
    "stores_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stores_val.shape)\n",
    "stores_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stores_test.shape)\n",
    "stores_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=stores_train['revenue'], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store_id and year are redundant as they provide no information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"store_name\"].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many unique store names, we omit this ATM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plaace Hierarchy ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"plaace_hierarchy_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"plaace_hierarchy_id_6\"] = stores_train[\"plaace_hierarchy_id\"].apply(lambda x: x[:3])\n",
    "stores_train[\"plaace_hierarchy_id_6\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA: split into 4 columns, first column contains only first number, second contains first two numbers...\n",
    "\n",
    "IDEA: split the 4 numbers into 4 columns.\n",
    "\n",
    "Treat as categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales Channel Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"sales_channel_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains same information as plaace hierarchy id, redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grunnkrets ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"grunnkrets_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply a foreign key to link to the other CSV files, will look at it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"address\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lat & Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stores_train[\"lon\"], stores_train[\"lat\"], \"bo\")\n",
    "plt.ylabel(\"lon\")\n",
    "plt.xlabel(\"lat\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)  # or 1000\n",
    "stores_train[\"chain_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"chain_name\"].isna().sum() / stores_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPUTE NANS: impute NANs as a category of its own. Can also treat the whole column as binary not-NAN/NAN. Can also decide threshold for when a chain becomes a NAN or another category altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mall Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"mall_name\"].isna().sum() / stores_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "stores_train[\"mall_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat similarly to chain name, we think a binary approach would be the best as the size of a lot of malls seems to be wrong due to missing shops etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.revenue.plot.hist(bins=50, logy=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(figsize=(12, 3), ncols=3)\n",
    "stores_train.isna().mean().plot.bar(ax=ax1)\n",
    "ax1.set_title('Fraction of rows with NaN values (train)')\n",
    "stores_test.isna().mean().plot.bar(ax=ax2)\n",
    "ax2.set_title('Fraction of rows with NaN values (test)')\n",
    "stores_train.revenue.plot.hist(bins=100, ax=ax3)\n",
    "ax3.set_title('Distribution of Revenues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by including lat, lon, chain_name, mall_name and plaace_hierarchy_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"is_mall\"] = ~stores_train[\"mall_name\"].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_limit = 10\n",
    "\n",
    "chain_count = stores_train[\"chain_name\"].value_counts().to_dict()\n",
    "stores_train[\"bounded_chain_name\"] = stores_train[\"chain_name\"].apply(lambda x: \"OTHER\" if(x in chain_count and chain_count[x] < lower_limit) else x)\n",
    "stores_train[[\"chain_name\", \"bounded_chain_name\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"plaace_cat_1\"] = stores_train[\"plaace_hierarchy_id\"].apply(lambda x: x[:1])\n",
    "stores_train[\"plaace_cat_2\"] = stores_train[\"plaace_hierarchy_id\"].apply(lambda x: x[:3])\n",
    "stores_train[\"plaace_cat_3\"] = stores_train[\"plaace_hierarchy_id\"].apply(lambda x: x[:5])\n",
    "stores_train[\"plaace_cat_4\"] = stores_train[\"plaace_hierarchy_id\"]\n",
    "stores_train[[\"plaace_cat_1\", \"plaace_cat_2\", \"plaace_cat_3\", \"plaace_cat_4\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"plaace_cat_4\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[stores_train[\"plaace_cat_1\"] == \"1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = stores_train[\"plaace_cat_\" + str(1)].unique()\n",
    "unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in unique_vals:\n",
    "    filtered_df = stores_train[stores_train[\"plaace_cat_1\"] == val]\n",
    "    filtered_df[\"random\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def closest_point(point, points):\n",
    "    \"\"\" Find closest point from a list of points. \"\"\"\n",
    "    if(len(points) == 0):\n",
    "        return None\n",
    "    return points[cdist([point], points).argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!NB**\n",
    "\n",
    "Next cell can take up to 1 minute to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"point\"] = [(x, y) for x,y in zip(stores_train['lat'], stores_train['lon'])]\n",
    "stores_train['closest'] = [closest_point(x[\"point\"], list(stores_train.loc[stores_train[\"plaace_cat_3\"] == x[\"plaace_cat_3\"]]['point'].drop([i], axis=0))) for i, x in stores_train.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"closest\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"closest\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in stores_train.iterrows():\n",
    "    if(row[\"closest\"] == None):\n",
    "        val = float(\"inf\")\n",
    "    else:\n",
    "        val = cdist(np.array(row[\"point\"]).reshape(1, -1), np.array(row[\"closest\"]).reshape(1, -1))\n",
    "    stores_train.at[i,'dist_to_nearest_comp'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_geographical_columns\n",
    "\n",
    "\n",
    "stores_train = create_geographical_columns(stores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_vals = stores_train[\"plaace_cat_4\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_dict = {}\n",
    "for val in unq_vals:\n",
    "    rev_dict[val] = stores_train[\"revenue\"].where(stores_train[\"plaace_cat_4\"] == val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train[\"mean_revenue\"] = stores_train[\"plaace_cat_4\"].apply(lambda x: rev_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_count = stores_train[\"chain_name\"].value_counts().to_dict()\n",
    "chain_count[np.nan] = 0\n",
    "lower_limit = 10\n",
    "\n",
    "rev_dict = {}\n",
    "mean_revenue = stores_train.revenue.mean()\n",
    "for val in unq_vals:\n",
    "    rev_dict[val] = stores_train[\"revenue\"].where(stores_train[\"plaace_cat_4\"] == val).mean()\n",
    "\n",
    "def generate_rev_dict(df, plaace_cat_granularity: int = 4):\n",
    "    rev_dict = {}\n",
    "    mean_revenue = df.revenue.mean()\n",
    "    for val in df[\"plaace_cat_\" + str(plaace_cat_granularity)]:\n",
    "        rev_dict[val] = df[\"revenue\"].where(df[\"plaace_cat_\" + str(plaace_cat_granularity)] == val).mean()\n",
    "    return rev_dict, mean_revenue\n",
    "\n",
    "def mean_func_rev(plaace_cat, rev_dict, mean_revenue):\n",
    "    if(plaace_cat in rev_dict.keys()):\n",
    "        return rev_dict[plaace_cat]\n",
    "    return mean_revenue\n",
    "\n",
    "def feature_engineer_df(\n",
    "    df: pd.DataFrame, \n",
    "    chain_count: dict, \n",
    "    rev_dict: dict, \n",
    "    training: bool = True, \n",
    "    training_df: pd.DataFrame = None, \n",
    "    lower_limit: int = 10, \n",
    "    plaace_cat_granularity: int = 4\n",
    "):\n",
    "    df[\"is_mall\"] = ~df[\"mall_name\"].isna()\n",
    "    df[\"is_chain\"] = ~df[\"chain_name\"].isna()\n",
    "    df[\"bounded_chain_name\"] = df[\"chain_name\"].apply(lambda x: \"OTHER\" if(x in chain_count and chain_count[x] < lower_limit) else x)\n",
    "    df[\"plaace_cat_1\"] = df[\"plaace_hierarchy_id\"].apply(lambda x: x[:1])\n",
    "    df[\"plaace_cat_2\"] = df[\"plaace_hierarchy_id\"].apply(lambda x: x[:3])\n",
    "    df[\"plaace_cat_3\"] = df[\"plaace_hierarchy_id\"].apply(lambda x: x[:5])\n",
    "    df[\"plaace_cat_4\"] = df[\"plaace_hierarchy_id\"]\n",
    "    df[\"point\"] = [(x, y) for x,y in zip(df['lat'], df['lon'])]\n",
    "    training_df[\"point\"] = [(x, y) for x,y in zip(training_df['lat'], training_df['lon'])]\n",
    "    if training:\n",
    "        df['closest_' + str(plaace_cat_granularity)] = [\n",
    "            closest_point(\n",
    "                x[\"point\"], \n",
    "                list(training_df.loc[\n",
    "                    training_df[\"plaace_cat_\" + str(plaace_cat_granularity)] == x[\"plaace_cat_\" + str(plaace_cat_granularity)]\n",
    "                    ]['point'].drop([i], axis=0))) for i, x in df.iterrows()\n",
    "            ]\n",
    "    else:\n",
    "        df['closest_' + str(plaace_cat_granularity)] = [\n",
    "            closest_point(\n",
    "                x[\"point\"], \n",
    "                list(training_df.loc[\n",
    "                    training_df[\"plaace_cat_\" + str(plaace_cat_granularity)] == x[\"plaace_cat_\" + str(plaace_cat_granularity)]\n",
    "                    ]['point'])) for i, x in df.iterrows()\n",
    "            ]\n",
    "    df[\"mean_revenue_\" + str(plaace_cat_granularity)] = df[\"plaace_cat_\" + str(plaace_cat_granularity)].apply(lambda x: mean_func_rev(x, rev_dict, mean_revenue))\n",
    "    for i, row in df.iterrows():\n",
    "        if(row[\"closest_\" + str(plaace_cat_granularity)] == None):\n",
    "            val = np.nan\n",
    "        else:\n",
    "            val = cdist(np.array(row[\"point\"]).reshape(1, -1), np.array(row[\"closest_\" + str(plaace_cat_granularity)]).reshape(1, -1))\n",
    "        df.at[i,'dist_to_nearest_comp'] = val\n",
    "    df = create_geographical_columns(df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_extra = pd.read_csv(\"data/stores_extra.csv\")\n",
    "stores_extra.index += stores_train.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([stores_train, stores_extra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    stores_train = feature_engineer_df(stores_train, chain_count, rev_dict, training_df=concat_df, plaace_cat_granularity=i)\n",
    "    stores_val = feature_engineer_df(stores_val, chain_count, rev_dict, training=False, training_df=concat_df, plaace_cat_granularity=i)\n",
    "    stores_test = feature_engineer_df(stores_test, chain_count, rev_dict, training=False, training_df=concat_df, plaace_cat_granularity=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.dist_to_nearest_comp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.dist_to_nearest_comp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data & training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by including lat, lon, chain_name, mall_name and plaace_hierarchy_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess_grunnkrets_df, create_geographical_columns\n",
    "\n",
    "class DataframeFunctionTransformer():\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        return self.func(input_df)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_population_df = pd.read_csv(\"temp_data/full_population_data_train.csv\")\n",
    "closest_bus_stop_df = pd.read_csv(\"temp_data/closest_bus_stops_train.csv\")\n",
    "\n",
    "full_stores_train = stores_train.merge(full_population_df, left_on=\"store_id\", right_on=\"store_id\")\n",
    "full_stores_train = full_stores_train.merge(closest_bus_stop_df, left_on=\"store_id\", right_on=\"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stores_val = stores_val.merge(full_population_df, left_on=\"store_id\", right_on=\"store_id\")\n",
    "full_stores_val = full_stores_val.merge(closest_bus_stop_df, left_on=\"store_id\", right_on=\"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_population_df_test = pd.read_csv(\"temp_data/full_population_data_test.csv\")\n",
    "closest_bus_stop_df_test = pd.read_csv(\"temp_data/closest_bus_stops_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stores_test = stores_test.merge(full_population_df_test, left_on=\"store_id\", right_on=\"store_id\")\n",
    "full_stores_test = full_stores_test.merge(closest_bus_stop_df_test, left_on=\"store_id\", right_on=\"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fylke_relevant_features = [col_name for col_name in full_stores_train.columns if col_name.startswith(\"fylke.\")]\n",
    "kommune_relevant_features = [col_name for col_name in full_stores_train.columns if col_name.startswith(\"kommune.\")]\n",
    "delomrade_relevant_features = [col_name for col_name in full_stores_train.columns if col_name.startswith(\"delomrade.\")]\n",
    "grunnkrets_relevant_features = [col_name for col_name in full_stores_train.columns if col_name.startswith(\"grunnkrets_id.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_delta = 1\n",
    "full_stores_train[\"log_revenue\"] = full_stores_train.revenue.apply(lambda x: np.log(x + log_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_removed_train = full_stores_train[full_stores_train[\"sales_channel_name\"] != \"Grocery stores\"]\n",
    "grocery_removed_val = full_stores_val[full_stores_val[\"sales_channel_name\"] != \"Grocery stores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_removed_train[\"log_revenue\"] = grocery_removed_train.revenue.apply(lambda x: np.log(x + log_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_removed_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stores_train[\"is_grocery\"] = full_stores_train.sales_channel_name.apply(lambda x: x == \"Grocery stores\")\n",
    "full_stores_val[\"is_grocery\"] = full_stores_val.sales_channel_name.apply(lambda x: x == \"Grocery stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stores_val.is_grocery.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv(\"temp_data/full_features_train.csv\", index_col=0)\n",
    "stores_val = pd.read_csv(\"temp_data/full_features_val.csv\", index_col=0)\n",
    "stores_extra = pd.read_csv(\"temp_data/full_features_extra.csv\", index_col=0)\n",
    "stores_test = pd.read_csv(\"temp_data/full_features_test.csv\", index_col=0)\n",
    "\n",
    "store_dataframes = {\n",
    "    \"train\": stores_train, \n",
    "    \"extra\": stores_extra, \n",
    "    \"test\": stores_test, \n",
    "    \"val\": stores_val\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "OE_categorical_features = [\"bounded_chain_name\", \"kommune\", \"delomrade\", \"is_grocery\", \"plaace_cat_3\", \"plaace_cat_4\"]\n",
    "OE_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\")),\n",
    "        (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "OH_categorical_features = [\"fylke\", \"plaace_cat_2\"]\n",
    "OH_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "numerical_features = [\"lat\", \"lon\", \"dist_to_nearest_comp\", \n",
    "\"mean_revenue_1\", \"mean_revenue_2\", \"mean_revenue_3\", \"mean_revenue_4\", \n",
    "] + delomrade_relevant_features + list(closest_bus_stop_df.columns[1:])\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True))]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"oe_cat\", OE_categorical_transformer, OE_categorical_features),\n",
    "        (\"oh_cat\", OH_categorical_transformer, OH_categorical_features),\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "X_train = preprocessor.fit_transform(full_stores_train)\n",
    "X_val = preprocessor.transform(full_stores_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(full_stores_train.log_revenue)\n",
    "y_val = np.array(full_stores_val.revenue)\n",
    "mean_y = y_train.mean()\n",
    "std_y = y_train.std()\n",
    "\n",
    "y_train -= mean_y\n",
    "y_train /= std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from RMSLE import rmsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=0, n_jobs=-1, n_estimators=250, max_features=8, min_samples_leaf=2, min_samples_split=16)\n",
    "rf_params = {\n",
    "    \"n_estimators\" : (100, 250, 500, 1000), \n",
    "    \"max_features\" : (1, 2, 4, 8), \n",
    "    \"min_samples_split\" : (4, 8, 16, 32), \n",
    "    \"min_samples_leaf\" : (2, 4, 8), \n",
    "    }\n",
    "\n",
    "rf_clf = GridSearchCV(rf, rf_params, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!NB Next cell takes several minutes to run (~5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_y_pred = rf.predict(X_val)\n",
    "rmsle(y_pred=rf_y_pred, y_true=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_clf = LinearRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_pred = lr_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_pred = np.array([max(0, xi) for xi in lr_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle(y_pred=lr_y_pred, y_true=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Current best params\n",
    "lgbm = LGBMRegressor(random_state=0, n_jobs=-1, learning_rate=0.1, n_estimators=100, reg_lambda=0.01)\n",
    "lgbm_params = {\n",
    "    #\"num_leaves\" : (10, 25, 31, 75), \n",
    "    \"learning_rate\" : (0.05, 0.1, 0.25), \n",
    "    \"n_estimators\" : (50, 100, 250), \n",
    "    #\"min_split_gain\" : (0, 0.01, 0.1), \n",
    "    #\"min_child_samples\" : (4, 8, 16, 32), \n",
    "    \"reg_alpha\" : (0, 0.01, 0.1), \n",
    "    \"reg_lambda\" : (0, 0.01, 0.1), \n",
    "    }\n",
    "\n",
    "lgbm_clf = GridSearchCV(lgbm, lgbm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!NB Depending on the total possible configurations of hyperparams, the next cell can take veeeeery long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_y_pred = lgbm.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_y_pred = np.array([max(0, xi) for xi in lgbm_y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle(y_pred=lgbm_y_pred, y_true=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "estimators = [\n",
    "    ('rf', rf), \n",
    "    ('lf', lr_clf), \n",
    "    ('lgbm', lgbm), \n",
    "]\n",
    "\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RandomForestRegressor(n_estimators=50, random_state=0, n_jobs=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_y_pred = reg.predict(X_val)\n",
    "reg_y_pred *= std_y\n",
    "reg_y_pred += mean_y\n",
    "reg_y_pred = np.exp(reg_y_pred) - log_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle(y_pred=reg_y_pred, y_true=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See largest contributors to high RMSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_diff = np.abs(reg_y_pred - y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_largest_diff = 100\n",
    "n_largest_index = np.argsort(-pred_diff)[:n_largest_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_index = list(full_stores_val.index[full_stores_val[\"sales_channel_name\"] == \"Grocery stores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_removed_y_pred = np.delete(reg_y_pred, grocery_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_removed_y_val = np.delete(y_val, grocery_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_largest_wrong_df = full_stores_val.iloc[n_largest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_largest_wrong_df.sales_channel_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set \n",
    "X_test = preprocessor.transform(full_stores_test)\n",
    "y_test_pred = reg.predict(X_test)\n",
    "y_test_pred *= std_y\n",
    "y_test_pred += mean_y\n",
    "y_test_pred = np.exp(y_test_pred) - log_delta\n",
    "\n",
    "# Generate submission dataframe \n",
    "# NOTE: It is important that the ID and predicted values match\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = stores_test.store_id \n",
    "submission['predicted'] = np.asarray(y_test_pred)\n",
    "\n",
    "# Save it to disk (`index=False` means don't save the index in the csv)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tdt4173')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b33d5bb6aa9988fd454a923e38ef42550e1626b613fe9fb888665b5922e892"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
