{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic to automatically update imports if functions in utils are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineer (option 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stores_train = pd.read_csv(\"data/stores_train.csv\")\n",
    "stores_test = pd.read_csv(\"data/stores_test.csv\")\n",
    "stores_extra = pd.read_csv(\"data/stores_extra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_plaace_cat\n",
    "\n",
    "\n",
    "store_dataframes = {\n",
    "    \"train\": stores_train, \n",
    "    \"extra\": stores_extra, \n",
    "    \"test\": stores_test, \n",
    "    }\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    df = split_plaace_cat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dataframes[\"train\"][\"log_revenue\"] = store_dataframes[\"train\"].revenue.apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 23.68it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 165.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import mean_rev_of_competitor, log_mean_rev_of_competitor, create_geographical_columns, create_chain_and_mall_columns, generate_chain_rev_dict, generate_plaace_rev_dict, create_mean_chain_rev_col\n",
    "\n",
    "chain_count = stores_train[\"chain_name\"].value_counts().to_dict()\n",
    "\n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    store_dataframes[df_name] = create_geographical_columns(df)\n",
    "    store_dataframes[df_name] = create_chain_and_mall_columns(df, chain_count, lower_limit=1)\n",
    "\n",
    "chain_rev_dict, log_bounded_chain_rev_dict = generate_chain_rev_dict(store_dataframes[\"train\"], quantile=0)\n",
    "\n",
    "for i in tqdm(range(1, 5)):\n",
    "    rev_plaace_dict, mean_plaace_revenue, log_rev_plaace_dict, log_mean_plaace_revenue = generate_plaace_rev_dict(store_dataframes[\"train\"], i, quantile=0)\n",
    "    for df_name, df in store_dataframes.items():\n",
    "            store_dataframes[df_name] = mean_rev_of_competitor(store_dataframes[df_name], i, rev_dict=rev_plaace_dict, mean_revenue=mean_plaace_revenue)\n",
    "            store_dataframes[df_name] = log_mean_rev_of_competitor(store_dataframes[df_name], i, log_rev_dict=log_rev_plaace_dict, log_mean_revenue=log_mean_plaace_revenue)\n",
    "    \n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    store_dataframes[df_name] = create_mean_chain_rev_col(df, bounded_chain_revs=chain_rev_dict, log_bounded_chain_revs=log_bounded_chain_rev_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import concat_df_keep_unq_index\n",
    "concat_df = concat_df_keep_unq_index(store_dataframes[\"train\"], store_dataframes[\"extra\"])\n",
    "concat_df = concat_df_keep_unq_index(concat_df, store_dataframes[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_dist_to_nearest_comp\n",
    "\n",
    "nearest_comp_plaace_cat_gran = [1, 2, 3, 4]\n",
    "n_nearest_comp = [1, 2, 3, 4, 5, 7, 10]\n",
    "\n",
    "store_dataframes[\"train\"] = find_dist_to_nearest_comp(\n",
    "    store_dataframes[\"train\"], \n",
    "    nearest_comp_plaace_cat_gran, \n",
    "    n_nearest_comp, \n",
    "    training=True, \n",
    "    training_df=concat_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dataframes[\"test\"] = find_dist_to_nearest_comp(\n",
    "    store_dataframes[\"test\"], \n",
    "    nearest_comp_plaace_cat_gran, \n",
    "    n_nearest_comp, \n",
    "    training=True,\n",
    "    training_df=concat_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plaace_cols = list(store_dataframes[\"train\"].columns[-56:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dict = store_dataframes[\"train\"][['log_revenue'] + comp_plaace_cols].corr().iloc[0].to_dict()\n",
    "sorted_relevant_dist_cols = [[k, v] for k, v in sorted(dist_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_relevant_cols = sorted_relevant_dist_cols[1:14:2]\n",
    "comp_relevant_cols = [r[0] for r in comp_relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [13:16<00:00, 265.54s/it]\n"
     ]
    }
   ],
   "source": [
    "from bus_utils import find_closest_bus_stop\n",
    "\n",
    "bus_stop_n = [1, 2, 3, 5, 7, 10, 15 ,25, 50, 100]\n",
    "bus_mean = True\n",
    "bus_sum = True\n",
    "\n",
    "bus_stop_columns = []\n",
    "\n",
    "if(bus_sum):\n",
    "    bus_stop_columns += [f\"closest_bus_stop_sum_{i}\" for i in bus_stop_n]\n",
    "\n",
    "if(bus_mean):\n",
    "    bus_stop_columns += [f\"closest_bus_stop_mean_{i}\" for i in bus_stop_n]\n",
    "\n",
    "\n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    store_dataframes[df_name] = find_closest_bus_stop(df, bus_stop_n, _sum=bus_sum, _mean=bus_mean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_dict = store_dataframes[\"train\"][['log_revenue'] + bus_stop_columns].corr().iloc[0].to_dict()\n",
    "bus_sorted_relevant_dist_cols = [[k, v] for k, v in sorted(bus_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_relevant_cols = bus_sorted_relevant_dist_cols[1::2]\n",
    "bus_relevant_cols = [r[0] for r in bus_relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_relevant_cols = bus_relevant_cols[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:07<00:15,  7.59s/it]/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      " 67%|██████▋   | 2/3 [00:22<00:11, 11.66s/it]/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density']  = full_population_df[f'{level}.total_income']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.income_density_log']  = np.log1p(full_population_df[f'{level}.income_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:104: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density'] = full_population_df[f'{level}.tot_pop']/full_population_df[f'{level}.area_km2']\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/grunnkrets.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_population_df[f'{level}.pop_density_log'] = np.log1p(full_population_df[f'{level}.pop_density'])\n",
      "100%|██████████| 3/3 [00:27<00:00,  9.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from grunnkrets_old import make_grunnkrets_df\n",
    "\n",
    "full_population_dataframes = {}\n",
    "full_pop_columns = []\n",
    "\n",
    "for df_name, df in tqdm(store_dataframes.items()):\n",
    "    full_population_dataframes[df_name] = make_grunnkrets_df(df)\n",
    "    full_pop_columns = full_population_dataframes[df_name].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_columns = list(full_pop_columns[-184:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = df.merge(\n",
    "        full_population_dataframes[df_name], \n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"outer\", \n",
    "        suffixes=('', '_redundant')\n",
    "    )\n",
    "    store_dataframes[df_name].drop(store_dataframes[df_name].filter(regex='_redundant$').columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_dict = store_dataframes[\"train\"][['log_revenue'] + full_pop_columns].corr().iloc[0].to_dict()\n",
    "full_pop_sorted_relevant_dist_cols = [[k, v] for k, v in sorted(full_pop_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_relevant_cols = full_pop_sorted_relevant_dist_cols[1:8]\n",
    "full_pop_relevant_cols = [r[0] for r in full_pop_relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fylke_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"fylke.\")]\n",
    "kommune_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"kommune.\")]\n",
    "delomrade_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"delomrade.\")]\n",
    "grunnkrets_relevant_features = [col_name for col_name in store_dataframes[\"train\"].columns if col_name.startswith(\"grunnkrets_id.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n",
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/num_stores.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped = geo_df.groupby([ level , cat]).sum()['count'].to_frame()\n"
     ]
    }
   ],
   "source": [
    "from num_stores import add_num_stores_info\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = add_num_stores_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_store_cols = list(store_dataframes[\"train\"].columns[-64:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_store_dict = store_dataframes[\"train\"][['log_revenue'] + num_store_cols].corr().iloc[0].to_dict()\n",
    "num_store_sorted_relevant_cols = [[k, v] for k, v in sorted(num_store_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_store_relevant_cols = num_store_sorted_relevant_cols[1:15]\n",
    "num_store_relevant_cols = [r[0] for r in num_store_relevant_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding mean revenue (plaace_cat, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avg_revenue import add_avg_revenue, create_avg_revenue_csvs\n",
    "\n",
    "create_avg_revenue_csvs()\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = add_avg_revenue(df, total=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_cols = list(store_dataframes[\"train\"].columns)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_dict = store_dataframes[\"train\"][['log_revenue'] + mean_rev_cols].corr().iloc[0].to_dict()\n",
    "mean_rev_sorted_relevant_cols = [[k, v] for k, v in sorted(mean_rev_dict.items(), key=lambda item: abs(item[1]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_relevant_cols = mean_rev_sorted_relevant_cols[1:]\n",
    "mean_rev_relevant_cols = [r[0] for r in mean_rev_relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rev_relevant_log_cols = []\n",
    "for col in mean_rev_relevant_cols:\n",
    "    for df_name, df in store_dataframes.items():\n",
    "        store_dataframes[df_name][col + \"_log\"] = store_dataframes[df_name][col].apply(lambda x: np.log1p(x))\n",
    "    mean_rev_relevant_log_cols.append(col + \"_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_plaace_index import create_index_csv, add_new_plaace_index\n",
    "\n",
    "create_index_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = add_new_plaace_index(store_dataframes[df_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oskarjor/NTNU/2022FALL/TDT4173/TDT4173/clustering.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  geo_df = stores_total_train.append(stores_extra).append(stores_test)[['lat', 'lon', 'store_id']]\n"
     ]
    }
   ],
   "source": [
    "from clustering import add_clusters, create_cluster_csv\n",
    "\n",
    "create_cluster_csv()\n",
    "\n",
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name] = add_clusters(store_dataframes[df_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in store_dataframes.items():\n",
    "    store_dataframes[df_name][\"chain_count\"] = store_dataframes[df_name].bounded_chain_name.apply(lambda x: 0 if (x == \"OTHER\" or x in chain_count.keys()) else chain_count[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_relevant_cols = [\n",
    "    'sum_dist_to_nearest_10_comp_plaace_1',\n",
    "    'mean_dist_to_nearest_7_comp_plaace_1',\n",
    "    'sum_dist_to_nearest_5_comp_plaace_1',\n",
    "    'sum_dist_to_nearest_4_comp_plaace_1',\n",
    "    'mean_dist_to_nearest_3_comp_plaace_1',\n",
    "    'sum_dist_to_nearest_2_comp_plaace_1',\n",
    "    'sum_dist_to_nearest_1_comp_plaace_1'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RMSLE import rmsle\n",
    "from pred_var_utils import reverse_log1p_transform_pred_var\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "OE_categorical_features = [\"bounded_chain_name\", \"kommune\", \"delomrade\", \"is_grocery\", \"plaace_cat_2\", \"plaace_cat_3\", \"plaace_cat_4\", \"grunnkrets_id\"]\n",
    "OE_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\")),\n",
    "        (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "OH_categorical_features = [\"plaace_cat_1\"]\n",
    "OH_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "numerical_features = [\"lat\", \"lon\", \n",
    "    \"log_mean_revenue_1\", \n",
    "    \"log_mean_revenue_2\", \n",
    "    \"log_mean_revenue_3\", \n",
    "    \"log_mean_revenue_4\", \n",
    "    \"log_chain_mean_revenue\"\n",
    "    ] + comp_relevant_cols \n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True))]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "       (\"oe_cat\", OE_categorical_transformer, OE_categorical_features),\n",
    "       (\"oh_cat\", OH_categorical_transformer, OH_categorical_features),\n",
    "       (\"num\", numerical_transformer, numerical_features),\n",
    "   ],\n",
    "   remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "X_train = preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "X_test = preprocessor.transform(store_dataframes[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(store_dataframes[\"train\"].log_revenue)\n",
    "mean_y = y_train.mean()\n",
    "std_y = y_train.std()\n",
    "\n",
    "y_train -= mean_y\n",
    "y_train /= std_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_cols_dict = {\n",
    "    \"OE\": [],\n",
    "    \"OH\": [],  \n",
    "    \"num\": []\n",
    "}\n",
    "\n",
    "reduntant_cols = [\"store_id\", \"year\", \"store_name\", \"plaace_hierarchy_id\", \"sales_channel_name\", \"address\", \"revenue\", \"log_revenue\", \"point\", \"plaace_cat_0\"]\n",
    "\n",
    "for col_name, dtype in store_dataframes[\"train\"].dtypes.to_dict().items():\n",
    "    if(col_name in reduntant_cols):\n",
    "        continue\n",
    "    if(dtype == int or dtype == float):\n",
    "        _type = \"num\"\n",
    "    elif(dtype == bool):\n",
    "        _type = \"OE\"\n",
    "    elif(dtype == object):\n",
    "        if(store_dataframes[\"train\"][col_name].nunique() <= 10):\n",
    "            _type = \"OH\"\n",
    "        else:\n",
    "            _type = \"OE\"\n",
    "    else:\n",
    "        print(f\"Unknown type {dtype} encountered for columns {col_name}\")\n",
    "    PCA_cols_dict[_type].append(col_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_num_cols = PCA_cols_dict[\"num\"][:339]\n",
    "PCA_cols_dict[\"OE\"] += PCA_cols_dict[\"num\"][-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_OH_cols = PCA_cols_dict[\"OH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_OE_cols = PCA_cols_dict[\"OE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full population cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_cols = all_num_cols[92:276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pop_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #(\"oe_cat\", OE_categorical_transformer, PCA_cols_dict[\"OE\"]),\n",
    "        #(\"oh_cat\", OH_categorical_transformer, PCA_cols_dict[\"OH\"]),\n",
    "        (\"num\", numerical_transformer, full_pop_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_PCA_X_train_full_pop_cols = full_pop_preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "pre_PCA_X_test_full_pop_cols = full_pop_preprocessor.transform(store_dataframes[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "PCA_X_train = pca.fit_transform(pre_PCA_X_train_full_pop_cols)\n",
    "PCA_X_test = pca.transform(pre_PCA_X_test_full_pop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7788648525545628\n",
      "[0.41349462 0.26895047 0.09641976]\n",
      "[989.11649004 797.71666919 477.6343187 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,PCA_X_train),axis=1)\n",
    "X_test = np.concatenate((X_test,PCA_X_test),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bus distance cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_cols = all_num_cols[72:92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #(\"oe_cat\", OE_categorical_transformer, PCA_cols_dict[\"OE\"]),\n",
    "        #(\"oh_cat\", OH_categorical_transformer, PCA_cols_dict[\"OH\"]),\n",
    "        (\"num\", numerical_transformer, bus_stop_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_PCA_X_train_bus_stop_cols = bus_stop_preprocessor.fit_transform(store_dataframes[\"train\"])\n",
    "pre_PCA_X_test_bus_stop_cols = bus_stop_preprocessor.fit_transform(store_dataframes[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "PCA_X_train = pca.fit_transform(pre_PCA_X_train_bus_stop_cols)\n",
    "PCA_X_test = pca.transform(pre_PCA_X_test_bus_stop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9445344957042299\n",
      "[0.79420438 0.15033011]\n",
      "[451.94411556 196.62629024]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,PCA_X_train),axis=1)\n",
    "X_test = np.concatenate((X_test,PCA_X_test),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# best params (8.11)\n",
    "# {'depth': 6, 'l2_leaf_reg': 10, 'learning_rate': 0.05, 'eval_metric': 'RMSE'}\n",
    "# rmsle(on val) = 0.0.7148919867904334\n",
    "\n",
    "# best params (12.11) (all features)\n",
    "# {'depth': 6, 'l2_leaf_reg': 10, 'learning_rate': 0.05, 'eval_metric': 'RMSE'}\n",
    "# rmsle(on val) = 0.7177413486698632\n",
    "\n",
    "cb = CatBoostRegressor(\n",
    "    random_seed=0, \n",
    "    verbose=False, \n",
    "    eval_metric=\"RMSE\", \n",
    "    rsm=0.1,\n",
    "    depth=8, \n",
    "    l2_leaf_reg= 8, \n",
    "    learning_rate= 0.03\n",
    "    )\n",
    "\n",
    "cb_params = grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.3],\n",
    "    'depth': [5, 6, 8, 10, 15, 20],\n",
    "    'l2_leaf_reg': [3, 4, 5, 6, 7, 8, 10, 15], \n",
    "    }\n",
    "\n",
    "#cb_clf = cb.randomized_search(cb_params, X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 8, 'l2_leaf_reg': 8, 'learning_rate': 0.03}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_clf[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1738079d0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set \n",
    "y_test_pred = reverse_log1p_transform_pred_var(cb.predict(X_test), std_y, mean_y)\n",
    "\n",
    "# Generate submission dataframe \n",
    "# NOTE: It is important that the ID and predicted values match\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = stores_test.store_id \n",
    "submission['predicted'] = np.asarray(y_test_pred)\n",
    "\n",
    "# Save it to disk (`index=False` means don't save the index in the csv)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tdt4173')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b33d5bb6aa9988fd454a923e38ef42550e1626b613fe9fb888665b5922e892"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
